# Streaming ASR Adapters (CTC) with Chunked Inference

This repo is a research codebase for **accuracy vs latency vs size** tradeoffs for CTC ASR, focused on:

- A from-scratch **log-mel + BiLSTM CTC** baseline (`asr/models/baseline_bilstm_ctc.py`)
- **Wav2Vec2ForCTC** fine-tuning:
  - full fine-tune (`wav2vec2_full`)
  - **LoRA PEFT** with frozen backbone (`wav2vec2_peft`) (`asr/models/wav2vec2_ctc.py`)
- Evaluation beyond offline WER:
  - **chunked/streaming simulation** via left-context + current-chunk re-encoding (`asr/eval/streaming.py`)
  - deployability probes: **fp16/fp32 + CPU int8 dynamic quantization** (`asr/eval/quant.py`)
  - optional **KenLM beam search** (`asr/decoding/lm_decoder.py`)

The committed outputs in `artifacts/` are for the **default smoke run** (`configs/smoke.yaml`) and are generated by:

- `artifacts/results.json` (machine-readable; produced by `scripts/eval.py`)
- `artifacts/report.md` (tables; produced by `scripts/report.py`)

## Problem Statement

How much word error rate (WER) do we give up to get a model that is:

- trainable with **parameter-efficient adapters** (LoRA),
- usable under a **streaming constraint** (finite chunk + left context),
- and **deployable** (reduced precision / quantization),

while keeping the experimental surface area small enough to iterate quickly.

## Dataset Provenance

This repo supports two dataset modes (`dataset.kind`):

- **Synthetic (smoke test)**: generated locally by `asr/data/librispeech.py::generate_synthetic_ctc_dataset`.
  - Audio: random noise with a mild envelope, written as WAV via `soundfile`.
  - Transcripts: random strings sampled from `dataset.vocab`, normalized with `asr/utils/text.py::normalize_text`.
  - Default sizes in `configs/smoke.yaml`: train=64, dev=16, test=16, durations in [0.6s, 1.2s].
- **LibriSpeech (real)**: downloaded from OpenSLR.
  - ASR audio+transcripts: OpenSLR resource 12 (LibriSpeech ASR), with MD5 verification from OpenSLR's `md5sum.txt` (`asr/data/librispeech.py::download_librispeech`).
  - Optional LM artifacts: OpenSLR resource 11 (3-gram ARPA + lexicon). OpenSLR does not publish checksums for these in this codepath.
  - Manifests: `*.jsonl` with `{utt_id, wav_path, text, speaker, chapter, duration_sec, sample_rate}` generated by `asr/data/librispeech.py::prepare_librispeech_manifests`.

## Methodology (What's Implemented)

- **Baseline** (`baseline`): log-mel frontend + per-utterance CMVN + BiLSTM + linear projection, trained with `torch.nn.CTCLoss` (`asr/train/train_baseline.py`).
- **Wav2Vec2 full** (`wav2vec2_full`): `transformers.Wav2Vec2ForCTC.from_pretrained(...)`, trained with HF's built-in CTC loss (`asr/train/train_wav2vec2.py`).
- **Wav2Vec2 + LoRA** (`wav2vec2_peft`): freeze backbone weights, inject LoRA modules into target Linear layers (configurable `target_modules`) (`asr/models/wav2vec2_ctc.py`, `asr/train/train_wav2vec2.py`).
  - Note: PEFT task-type API differences are handled by returning the underlying `base_model` wrapper to preserve `input_values=...` forwarding.

Evaluation (`asr/eval/runner.py`):

- **WER**: `jiwer.wer` on normalized reference/hypothesis strings (`asr/eval/metrics.py`). (WER can exceed 1.0 if insertions dominate.)
- **Streaming simulation** (wav2vec2 only): for each chunk, run the model on `[left_context + chunk]`, then drop logits attributable to the left-context proportion and concatenate (`asr/eval/streaming.py`).
  - Reports **RTF** (wall time / audio duration) and mean per-chunk latency (ms), excluding configurable warmup chunks.
- **Quantization / precision**:
  - `int8_dynamic_cpu`: `torch.ao.quantization.quantize_dynamic(..., {nn.Linear})` (CPU-only).
  - `fp16_cuda`: only if running on CUDA.
  - LoRA PEFT checkpoints are **skipped for dynamic quantization** due to PEFT module dtype expectations (`asr/eval/runner.py`).

## Baselines / Ablations

Implemented experiment axes (all driven by YAML in `configs/` and recorded verbatim in `artifacts/results.json:config`):

- Model family: `baseline` vs `wav2vec2_full` vs `wav2vec2_peft`
- PEFT knobs: LoRA `r/alpha/dropout/target_modules` (see `configs/smoke.yaml:training.wav2vec2_peft.lora`)
- Decoding: `greedy` (default) vs optional `kenlm` beam search (`eval.decoding`)
- Streaming sweep (wav2vec2): `eval.streaming.chunk_sec` x `eval.streaming.left_context_sec`
- Quant/precision sweep: `eval.quantization.schemes` (e.g., `fp32`, `fp16_cuda`, `int8_dynamic_cpu`)

## Results (Committed Smoke Run)

These numbers are from `artifacts/report.md` (generated from `artifacts/results.json`) for `run_name: smoke` (`configs/smoke.yaml`) on **CPU-only** (`torch==2.10.0+cpu`, `python==3.12.3`).

Because the smoke dataset is random noise with random transcripts and the wav2vec2 checkpoint is `hf-internal-testing/tiny-random-wav2vec2`, these results validate the pipeline, not ASR quality.

Smoke config highlights (from `artifacts/results.json:config`):

- Dataset: synthetic; train/dev/test = 64/16/16; durations in [0.6s, 1.2s]; vocab = `["a","b","c","d","e"," "]`
- Wav2vec2 checkpoint: `hf-internal-testing/tiny-random-wav2vec2`
- LoRA: `r=4`, `alpha=8`, `dropout=0.05`, `target_modules=["q_proj","k_proj","v_proj","out_proj","fc1","fc2"]`
- Streaming sweep: `chunk_sec=[0.5]`, `left_context_sec=[0.0, 1.0]` (8 utts per split)
- Quant sweep: `schemes=["fp32","int8_dynamic_cpu"]` (8 utts per split)

WER (offline, greedy):

| model | dev WER (N=16) | test WER (N=16) |
| --- | --- | --- |
| baseline | 1.0370 | 1.0000 |
| wav2vec2_full | 1.0000 | 1.0000 |
| wav2vec2_peft | 6.5926 | 7.3750 |

Streaming simulation (chunk=0.5s; greedy; `eval.streaming.max_utts_per_split=8`):

| model | split | left context (s) | WER | RTF | avg chunk (ms) |
| --- | --- | --- | --- | --- | --- |
| wav2vec2_full | dev | 0.0 | 1.0000 | 0.0071 | 2.4938 |
| wav2vec2_full | dev | 1.0 | 1.0000 | 0.0079 | 3.2729 |
| wav2vec2_full | test | 0.0 | 1.0000 | 0.0063 | 2.3710 |
| wav2vec2_full | test | 1.0 | 1.0000 | 0.0074 | 3.2628 |
| wav2vec2_peft | dev | 0.0 | 5.5909 | 0.0094 | 3.4882 |
| wav2vec2_peft | dev | 1.0 | 5.6364 | 0.0103 | 4.0561 |
| wav2vec2_peft | test | 0.0 | 4.8095 | 0.0091 | 3.4898 |
| wav2vec2_peft | test | 1.0 | 4.9524 | 0.0099 | 4.0659 |

Quantization / precision (greedy; `eval.quantization.max_utts_per_split=8`):

| model | split | scheme | WER | RTF | size (MB) |
| --- | --- | --- | --- | --- | --- |
| baseline | dev | fp32 | 1.0000 | 0.0067 | 2.4049 |
| baseline | dev | int8_dynamic_cpu | 1.0000 | 0.0052 | 2.3998 |
| baseline | test | fp32 | 1.0000 | 0.0056 | 2.4049 |
| baseline | test | int8_dynamic_cpu | 1.0000 | 0.0051 | 2.3998 |
| wav2vec2_full | dev | fp32 | 1.0000 | 0.0040 | 0.1364 |
| wav2vec2_full | dev | int8_dynamic_cpu | 1.0000 | 0.0041 | 0.1343 |
| wav2vec2_full | test | fp32 | 1.0000 | 0.0039 | 0.1364 |
| wav2vec2_full | test | int8_dynamic_cpu | 1.0000 | 0.0041 | 0.1343 |
| wav2vec2_peft | dev | fp32 | 5.8182 | 0.0046 | 0.1694 |
| wav2vec2_peft | test | fp32 | 5.0476 | 0.0046 | 0.1694 |

For the canonical tables (and the exact per-subset groupings), see:

- `artifacts/report.md` section **WER**
- `artifacts/report.md` section **Streaming (Chunked Inference)**
- `artifacts/report.md` section **Quantization / Precision**

This run does not include plotted figures; the report is Markdown tables, and the source-of-truth is the structured metrics in `artifacts/results.json` under `experiments.*`.

## Repro Instructions

Smoke (reproduces the same pipeline structure; exact WER may vary with dependency versions/hardware):

```bash
make all
```

Key outputs:

- `artifacts/checkpoints/<run_name>/{baseline,wav2vec2_full,wav2vec2_peft}/best.pt`
- `artifacts/results.json`
- `artifacts/report.md`

Useful partial runs:

```bash
make data
make baseline_train
make model_train_full
make model_train_peft
make eval
make report
```

Full LibriSpeech run (downloads multi-GB; manifests in `data/manifests/`):

```bash
make all CONFIG=configs/full.yaml
```

Optional KenLM decoding:

```bash
.venv/bin/pip install -r requirements-lm.txt
```

Then set in your config:

- `eval.decoding: ["greedy", "kenlm"]`
- `dataset.download_lm: true`
- `lm.arpa_gz: data/raw/lm/3-gram.pruned.1e-7.arpa.gz`

and rerun `make data eval report`.

## Limitations

- The committed results are a **smoke test** on synthetic noise; do not interpret the WER tables as meaningful ASR performance.
- The streaming evaluation is an **approximation**: it re-encodes left-context for every chunk and slices logits proportionally (no caching of conv features/transformer states).
- Dynamic int8 quantization is **Linear-only** and **CPU-only** in this implementation; it does not reflect full int8 deployment paths.
- LoRA PEFT checkpoints are **not compatible** with the current dynamic quantization path (explicitly skipped).
- Audio loading uses `soundfile` and a simple linear resampler for mismatched sample rates (`asr/data/dataset.py`), not a production resampler.

## Next Research Steps

1. Run the full pipeline on LibriSpeech with a real CTC checkpoint (e.g., `facebook/wav2vec2-base-960h` in `configs/full.yaml`) and report clean/other splits separately.
2. Replace the streaming approximation with **stateful streaming** (feature-extractor and transformer cache) and report latency at fixed endpointing constraints.
3. Add a decoding sweep: KenLM beam parameters (alpha/beta/beam width) and vocabulary/lexicon alignment.
4. Expand adapter ablations: LoRA rank/targets, adapters vs prefix-tuning, freezing subsets of layers, and compute trainable-parameter counts.
5. Add deployment-focused exports and metrics: ONNX/TorchScript, operator coverage, and int8 paths beyond dynamic quantization (static/QAT).
