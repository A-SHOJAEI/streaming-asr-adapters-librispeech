
[2026-02-10 19:38:37 UTC] autofix: patched Makefile setup for venv bootstrap/PEP668

[2026-02-10 19:38:37 UTC] $ make setup
Collecting pip
  Using cached pip-26.0.1-py3-none-any.whl.metadata (4.7 kB)
Using cached pip-26.0.1-py3-none-any.whl (1.8 MB)
Installing collected packages: pip
Successfully installed pip-26.0.1
bash scripts/bootstrap_venv.sh
Requirement already satisfied: pip in ./.venv/lib/python3.12/site-packages (26.0.1)
Collecting setuptools
  Using cached setuptools-82.0.0-py3-none-any.whl.metadata (6.6 kB)
Collecting wheel
  Using cached wheel-0.46.3-py3-none-any.whl.metadata (2.4 kB)
Collecting packaging>=24.0 (from wheel)
  Using cached packaging-26.0-py3-none-any.whl.metadata (3.3 kB)
Using cached setuptools-82.0.0-py3-none-any.whl (1.0 MB)
Using cached wheel-0.46.3-py3-none-any.whl (30 kB)
Using cached packaging-26.0-py3-none-any.whl (74 kB)
Installing collected packages: setuptools, packaging, wheel

Successfully installed packaging-26.0 setuptools-82.0.0 wheel-0.46.3
Collecting torch==2.10.0 (from -r requirements.txt (line 2))
  Using cached torch-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (31 kB)
Collecting torchaudio==2.10.0 (from -r requirements.txt (line 3))
  Downloading torchaudio-2.10.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (6.9 kB)
Collecting transformers==4.57.6 (from -r requirements.txt (line 6))
  Using cached transformers-4.57.6-py3-none-any.whl.metadata (43 kB)
Collecting peft==0.17.1 (from -r requirements.txt (line 7))
  Downloading peft-0.17.1-py3-none-any.whl.metadata (14 kB)
Collecting accelerate==1.12.0 (from -r requirements.txt (line 8))
  Using cached accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)
Collecting huggingface_hub==1.4.1 (from -r requirements.txt (line 9))
  Using cached huggingface_hub-1.4.1-py3-none-any.whl.metadata (13 kB)
Collecting safetensors==0.7.0 (from -r requirements.txt (line 10))
  Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)
Collecting tokenizers==0.22.2 (from -r requirements.txt (line 11))
  Using cached tokenizers-0.22.2-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.3 kB)
Collecting numpy==2.2.6 (from -r requirements.txt (line 14))
  Downloading numpy-2.2.6-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)
Collecting soundfile==0.13.1 (from -r requirements.txt (line 15))
  Downloading soundfile-0.13.1-py2.py3-none-manylinux_2_28_x86_64.whl.metadata (16 kB)
Collecting jiwer==4.0.0 (from -r requirements.txt (line 16))
  Downloading jiwer-4.0.0-py3-none-any.whl.metadata (3.3 kB)
Collecting PyYAML==6.0.3 (from -r requirements.txt (line 19))
  Downloading pyyaml-6.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.4 kB)
Collecting requests==2.32.5 (from -r requirements.txt (line 20))
  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)
Collecting tqdm==4.67.3 (from -r requirements.txt (line 21))
  Downloading tqdm-4.67.3-py3-none-any.whl.metadata (57 kB)
Requirement already satisfied: packaging==26.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 22)) (26.0)
Collecting filelock (from torch==2.10.0->-r requirements.txt (line 2))
  Using cached filelock-3.20.3-py3-none-any.whl.metadata (2.1 kB)
Collecting typing-extensions>=4.10.0 (from torch==2.10.0->-r requirements.txt (line 2))
  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)
Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch==2.10.0->-r requirements.txt (line 2)) (82.0.0)
Collecting sympy>=1.13.3 (from torch==2.10.0->-r requirements.txt (line 2))
  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)
Collecting networkx>=2.5.1 (from torch==2.10.0->-r requirements.txt (line 2))
  Using cached networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)
Collecting jinja2 (from torch==2.10.0->-r requirements.txt (line 2))
  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)
Collecting fsspec>=0.8.5 (from torch==2.10.0->-r requirements.txt (line 2))
  Using cached fsspec-2026.2.0-py3-none-any.whl.metadata (10 kB)
Collecting cuda-bindings==12.9.4 (from torch==2.10.0->-r requirements.txt (line 2))
  Using cached cuda_bindings-12.9.4-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (2.6 kB)
Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.10.0->-r requirements.txt (line 2))
  Using cached nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.10.0->-r requirements.txt (line 2))
  Using cached nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.10.0->-r requirements.txt (line 2))
  Using cached nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cudnn-cu12==9.10.2.21 (from torch==2.10.0->-r requirements.txt (line 2))
  Using cached nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.10.0->-r requirements.txt (line 2))
  Using cached nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.10.0->-r requirements.txt (line 2))
  Using cached nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-curand-cu12==10.3.9.90 (from torch==2.10.0->-r requirements.txt (line 2))
  Using cached nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.10.0->-r requirements.txt (line 2))
  Using cached nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.10.0->-r requirements.txt (line 2))
  Using cached nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-cusparselt-cu12==0.7.1 (from torch==2.10.0->-r requirements.txt (line 2))
  Using cached nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl.metadata (7.0 kB)
Collecting nvidia-nccl-cu12==2.27.5 (from torch==2.10.0->-r requirements.txt (line 2))
  Using cached nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)
Collecting nvidia-nvshmem-cu12==3.4.5 (from torch==2.10.0->-r requirements.txt (line 2))
  Using cached nvidia_nvshmem_cu12-3.4.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)
Collecting nvidia-nvtx-cu12==12.8.90 (from torch==2.10.0->-r requirements.txt (line 2))
  Using cached nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)
Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch==2.10.0->-r requirements.txt (line 2))
  Using cached nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)
Collecting nvidia-cufile-cu12==1.13.1.3 (from torch==2.10.0->-r requirements.txt (line 2))
  Using cached nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)
Collecting triton==3.6.0 (from torch==2.10.0->-r requirements.txt (line 2))
  Using cached triton-3.6.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)
INFO: pip is looking at multiple versions of transformers to determine which version is compatible with other requirements. This could take a while.
ERROR: Cannot install -r requirements.txt (line 6) and huggingface_hub==1.4.1 because these package versions have conflicting dependencies.

The conflict is caused by:
    The user requested huggingface_hub==1.4.1
    transformers 4.57.6 depends on huggingface-hub<1.0 and >=0.34.0

Additionally, some packages in these conflicts have no matching distributions available for your environment:
    huggingface-hub

To fix this you could try to:
1. loosen the range of package versions you've specified
2. remove package versions to allow pip to attempt to solve the dependency conflict

ERROR: ResolutionImpossible: for help visit https://pip.pypa.io/en/latest/topics/dependency-resolution/#dealing-with-dependency-conflicts
make: *** [Makefile:20: setup] Error 1

[2026-02-10 19:54:53 UTC] $ make all
bash scripts/bootstrap_venv.sh
Requirement already satisfied: pip in ./.venv/lib/python3.12/site-packages (26.0.1)
Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (82.0.0)
Requirement already satisfied: wheel in ./.venv/lib/python3.12/site-packages (0.46.3)
Requirement already satisfied: packaging>=24.0 in ./.venv/lib/python3.12/site-packages (from wheel) (26.0)
Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cpu
Requirement already satisfied: torch==2.10.0+cpu in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 3)) (2.10.0+cpu)
Requirement already satisfied: torchaudio==2.10.0+cpu in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 4)) (2.10.0+cpu)
Requirement already satisfied: transformers==4.57.6 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 7)) (4.57.6)
Requirement already satisfied: peft==0.18.1 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (0.18.1)
Requirement already satisfied: accelerate==1.12.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 9)) (1.12.0)
Requirement already satisfied: huggingface-hub==0.34.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 10)) (0.34.0)
Requirement already satisfied: safetensors==0.7.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 11)) (0.7.0)
Requirement already satisfied: tokenizers==0.22.2 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 12)) (0.22.2)
Requirement already satisfied: numpy==2.2.6 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 15)) (2.2.6)
Requirement already satisfied: soundfile==0.13.1 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 16)) (0.13.1)
Requirement already satisfied: jiwer==4.0.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 17)) (4.0.0)
Requirement already satisfied: PyYAML==6.0.3 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 20)) (6.0.3)
Requirement already satisfied: requests==2.32.5 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 21)) (2.32.5)
Requirement already satisfied: tqdm==4.67.3 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 22)) (4.67.3)
Requirement already satisfied: packaging==26.0 in ./.venv/lib/python3.12/site-packages (from -r requirements.txt (line 23)) (26.0)
Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch==2.10.0+cpu->-r requirements.txt (line 3)) (3.20.3)
Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch==2.10.0+cpu->-r requirements.txt (line 3)) (4.15.0)
Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch==2.10.0+cpu->-r requirements.txt (line 3)) (82.0.0)
Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch==2.10.0+cpu->-r requirements.txt (line 3)) (1.14.0)
Requirement already satisfied: networkx>=2.5.1 in ./.venv/lib/python3.12/site-packages (from torch==2.10.0+cpu->-r requirements.txt (line 3)) (3.6.1)
Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch==2.10.0+cpu->-r requirements.txt (line 3)) (3.1.6)
Requirement already satisfied: fsspec>=0.8.5 in ./.venv/lib/python3.12/site-packages (from torch==2.10.0+cpu->-r requirements.txt (line 3)) (2026.2.0)
Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers==4.57.6->-r requirements.txt (line 7)) (2026.1.15)
Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.12/site-packages (from huggingface-hub==0.34.0->-r requirements.txt (line 10)) (1.2.0)
Requirement already satisfied: psutil in ./.venv/lib/python3.12/site-packages (from peft==0.18.1->-r requirements.txt (line 8)) (7.2.2)
Requirement already satisfied: cffi>=1.0 in ./.venv/lib/python3.12/site-packages (from soundfile==0.13.1->-r requirements.txt (line 16)) (2.0.0)
Requirement already satisfied: click>=8.1.8 in ./.venv/lib/python3.12/site-packages (from jiwer==4.0.0->-r requirements.txt (line 17)) (8.3.1)
Requirement already satisfied: rapidfuzz>=3.9.7 in ./.venv/lib/python3.12/site-packages (from jiwer==4.0.0->-r requirements.txt (line 17)) (3.14.3)
Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests==2.32.5->-r requirements.txt (line 21)) (3.4.4)
Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests==2.32.5->-r requirements.txt (line 21)) (3.11)
Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests==2.32.5->-r requirements.txt (line 21)) (2.6.3)
Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests==2.32.5->-r requirements.txt (line 21)) (2026.1.4)
Requirement already satisfied: pycparser in ./.venv/lib/python3.12/site-packages (from cffi>=1.0->soundfile==0.13.1->-r requirements.txt (line 16)) (3.0)
Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch==2.10.0+cpu->-r requirements.txt (line 3)) (1.3.0)
Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch==2.10.0+cpu->-r requirements.txt (line 3)) (3.0.3)
.venv/bin/python -m scripts.sanity
{
  "cuda_available": false,
  "python": "3.12.3",
  "torch": "2.10.0+cpu"
}
.venv/bin/python -m scripts.data --config configs/smoke.yaml
.venv/bin/python -m scripts.train --config configs/smoke.yaml
baseline epoch 1/1:   0%|          | 0/8 [00:00<?, ?it/s]/home/alireza/research-autopilot/generated_projects/streaming-asr-adapters-librispeech/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1118: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
  super().__init__(loader)
baseline epoch 1/1:   0%|          | 0/8 [00:00<?, ?it/s, loss=14]baseline epoch 1/1:  12%|█▎        | 1/8 [00:00<00:00,  8.67it/s, loss=14]baseline epoch 1/1:  12%|█▎        | 1/8 [00:00<00:00,  8.67it/s, loss=16.5]baseline epoch 1/1:  12%|█▎        | 1/8 [00:00<00:00,  8.67it/s, loss=10.9]baseline epoch 1/1:  12%|█▎        | 1/8 [00:00<00:00,  8.67it/s, loss=8.34]baseline epoch 1/1:  50%|█████     | 4/8 [00:00<00:00, 18.21it/s, loss=8.34]baseline epoch 1/1:  50%|█████     | 4/8 [00:00<00:00, 18.21it/s, loss=6.51]baseline epoch 1/1:  50%|█████     | 4/8 [00:00<00:00, 18.21it/s, loss=5.83]baseline epoch 1/1:  50%|█████     | 4/8 [00:00<00:00, 18.21it/s, loss=3.85]baseline epoch 1/1:  88%|████████▊ | 7/8 [00:00<00:00, 22.21it/s, loss=3.85]baseline epoch 1/1:  88%|████████▊ | 7/8 [00:00<00:00, 22.21it/s, loss=3.75]baseline epoch 1/1: 100%|██████████| 8/8 [00:00<00:00, 20.25it/s, loss=3.75]
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 and are newly initialized: ['wav2vec2.feature_extractor.conv_layers.1.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.1.layer_norm.weight', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wav2vec2_full epoch 1/1:   0%|          | 0/32 [00:00<?, ?it/s]wav2vec2_full epoch 1/1:   0%|          | 0/32 [00:00<?, ?it/s, loss=1.42e+3]wav2vec2_full epoch 1/1:   3%|▎         | 1/32 [00:00<00:03,  8.30it/s, loss=1.42e+3]wav2vec2_full epoch 1/1:   3%|▎         | 1/32 [00:00<00:03,  8.30it/s, loss=1.54e+3]wav2vec2_full epoch 1/1:   3%|▎         | 1/32 [00:00<00:03,  8.30it/s, loss=1.34e+3]wav2vec2_full epoch 1/1:   3%|▎         | 1/32 [00:00<00:03,  8.30it/s, loss=1.65e+3]wav2vec2_full epoch 1/1:  12%|█▎        | 4/32 [00:00<00:01, 15.13it/s, loss=1.65e+3]wav2vec2_full epoch 1/1:  12%|█▎        | 4/32 [00:00<00:01, 15.13it/s, loss=1.08e+3]wav2vec2_full epoch 1/1:  12%|█▎        | 4/32 [00:00<00:01, 15.13it/s, loss=1.34e+3]wav2vec2_full epoch 1/1:  12%|█▎        | 4/32 [00:00<00:01, 15.13it/s, loss=1.79e+3]wav2vec2_full epoch 1/1:  22%|██▏       | 7/32 [00:00<00:01, 19.89it/s, loss=1.79e+3]wav2vec2_full epoch 1/1:  22%|██▏       | 7/32 [00:00<00:01, 19.89it/s, loss=1.77e+3]wav2vec2_full epoch 1/1:  22%|██▏       | 7/32 [00:00<00:01, 19.89it/s, loss=1.13e+3]wav2vec2_full epoch 1/1:  22%|██▏       | 7/32 [00:00<00:01, 19.89it/s, loss=1.43e+3]wav2vec2_full epoch 1/1:  31%|███▏      | 10/32 [00:00<00:00, 22.99it/s, loss=1.43e+3]wav2vec2_full epoch 1/1:  31%|███▏      | 10/32 [00:00<00:00, 22.99it/s, loss=1.78e+3]wav2vec2_full epoch 1/1:  31%|███▏      | 10/32 [00:00<00:00, 22.99it/s, loss=1.54e+3]wav2vec2_full epoch 1/1:  31%|███▏      | 10/32 [00:00<00:00, 22.99it/s, loss=1.55e+3]wav2vec2_full epoch 1/1:  41%|████      | 13/32 [00:00<00:00, 22.83it/s, loss=1.55e+3]wav2vec2_full epoch 1/1:  41%|████      | 13/32 [00:00<00:00, 22.83it/s, loss=1.48e+3]wav2vec2_full epoch 1/1:  41%|████      | 13/32 [00:00<00:00, 22.83it/s, loss=1.08e+3]wav2vec2_full epoch 1/1:  41%|████      | 13/32 [00:00<00:00, 22.83it/s, loss=1.15e+3]wav2vec2_full epoch 1/1:  41%|████      | 13/32 [00:00<00:00, 22.83it/s, loss=1.27e+3]wav2vec2_full epoch 1/1:  53%|█████▎    | 17/32 [00:00<00:00, 25.31it/s, loss=1.27e+3]wav2vec2_full epoch 1/1:  53%|█████▎    | 17/32 [00:00<00:00, 25.31it/s, loss=1.58e+3]wav2vec2_full epoch 1/1:  53%|█████▎    | 17/32 [00:00<00:00, 25.31it/s, loss=1.4e+3] wav2vec2_full epoch 1/1:  53%|█████▎    | 17/32 [00:00<00:00, 25.31it/s, loss=1.35e+3]wav2vec2_full epoch 1/1:  53%|█████▎    | 17/32 [00:00<00:00, 25.31it/s, loss=1.27e+3]wav2vec2_full epoch 1/1:  66%|██████▌   | 21/32 [00:00<00:00, 27.36it/s, loss=1.27e+3]wav2vec2_full epoch 1/1:  66%|██████▌   | 21/32 [00:00<00:00, 27.36it/s, loss=1.22e+3]wav2vec2_full epoch 1/1:  66%|██████▌   | 21/32 [00:00<00:00, 27.36it/s, loss=1.25e+3]wav2vec2_full epoch 1/1:  66%|██████▌   | 21/32 [00:00<00:00, 27.36it/s, loss=1.46e+3]wav2vec2_full epoch 1/1:  75%|███████▌  | 24/32 [00:00<00:00, 27.92it/s, loss=1.46e+3]wav2vec2_full epoch 1/1:  75%|███████▌  | 24/32 [00:01<00:00, 27.92it/s, loss=979]    wav2vec2_full epoch 1/1:  75%|███████▌  | 24/32 [00:01<00:00, 27.92it/s, loss=1.13e+3]wav2vec2_full epoch 1/1:  75%|███████▌  | 24/32 [00:01<00:00, 27.92it/s, loss=1.38e+3]wav2vec2_full epoch 1/1:  75%|███████▌  | 24/32 [00:01<00:00, 27.92it/s, loss=1.46e+3]wav2vec2_full epoch 1/1:  88%|████████▊ | 28/32 [00:01<00:00, 29.31it/s, loss=1.46e+3]wav2vec2_full epoch 1/1:  88%|████████▊ | 28/32 [00:01<00:00, 29.31it/s, loss=1.48e+3]wav2vec2_full epoch 1/1:  88%|████████▊ | 28/32 [00:01<00:00, 29.31it/s, loss=1.15e+3]wav2vec2_full epoch 1/1:  88%|████████▊ | 28/32 [00:01<00:00, 29.31it/s, loss=1.31e+3]wav2vec2_full epoch 1/1:  97%|█████████▋| 31/32 [00:01<00:00, 26.68it/s, loss=1.31e+3]wav2vec2_full epoch 1/1:  97%|█████████▋| 31/32 [00:01<00:00, 26.68it/s, loss=1.42e+3]wav2vec2_full epoch 1/1: 100%|██████████| 32/32 [00:01<00:00, 24.39it/s, loss=1.42e+3]
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 and are newly initialized: ['wav2vec2.feature_extractor.conv_layers.1.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.1.layer_norm.weight', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
wav2vec2_peft epoch 1/1:   0%|          | 0/32 [00:00<?, ?it/s]wav2vec2_peft epoch 1/1:   0%|          | 0/32 [00:00<?, ?it/s, loss=1.2e+3]wav2vec2_peft epoch 1/1:   3%|▎         | 1/32 [00:00<00:04,  7.66it/s, loss=1.2e+3]wav2vec2_peft epoch 1/1:   3%|▎         | 1/32 [00:00<00:04,  7.66it/s, loss=1.25e+3]wav2vec2_peft epoch 1/1:   3%|▎         | 1/32 [00:00<00:04,  7.66it/s, loss=1.55e+3]wav2vec2_peft epoch 1/1:   3%|▎         | 1/32 [00:00<00:04,  7.66it/s, loss=1.21e+3]wav2vec2_peft epoch 1/1:  12%|█▎        | 4/32 [00:00<00:01, 18.11it/s, loss=1.21e+3]wav2vec2_peft epoch 1/1:  12%|█▎        | 4/32 [00:00<00:01, 18.11it/s, loss=1.57e+3]wav2vec2_peft epoch 1/1:  12%|█▎        | 4/32 [00:00<00:01, 18.11it/s, loss=1.29e+3]wav2vec2_peft epoch 1/1:  12%|█▎        | 4/32 [00:00<00:01, 18.11it/s, loss=1.6e+3] wav2vec2_peft epoch 1/1:  22%|██▏       | 7/32 [00:00<00:01, 22.69it/s, loss=1.6e+3]wav2vec2_peft epoch 1/1:  22%|██▏       | 7/32 [00:00<00:01, 22.69it/s, loss=1.52e+3]wav2vec2_peft epoch 1/1:  22%|██▏       | 7/32 [00:00<00:01, 22.69it/s, loss=1.1e+3] wav2vec2_peft epoch 1/1:  22%|██▏       | 7/32 [00:00<00:01, 22.69it/s, loss=1.59e+3]wav2vec2_peft epoch 1/1:  31%|███▏      | 10/32 [00:00<00:00, 25.14it/s, loss=1.59e+3]wav2vec2_peft epoch 1/1:  31%|███▏      | 10/32 [00:00<00:00, 25.14it/s, loss=1.58e+3]wav2vec2_peft epoch 1/1:  31%|███▏      | 10/32 [00:00<00:00, 25.14it/s, loss=1.27e+3]wav2vec2_peft epoch 1/1:  31%|███▏      | 10/32 [00:00<00:00, 25.14it/s, loss=1.39e+3]wav2vec2_peft epoch 1/1:  41%|████      | 13/32 [00:00<00:00, 26.67it/s, loss=1.39e+3]wav2vec2_peft epoch 1/1:  41%|████      | 13/32 [00:00<00:00, 26.67it/s, loss=1.29e+3]wav2vec2_peft epoch 1/1:  41%|████      | 13/32 [00:00<00:00, 26.67it/s, loss=1.45e+3]wav2vec2_peft epoch 1/1:  41%|████      | 13/32 [00:00<00:00, 26.67it/s, loss=984]    wav2vec2_peft epoch 1/1:  41%|████      | 13/32 [00:00<00:00, 26.67it/s, loss=1.61e+3]wav2vec2_peft epoch 1/1:  53%|█████▎    | 17/32 [00:00<00:00, 29.63it/s, loss=1.61e+3]wav2vec2_peft epoch 1/1:  53%|█████▎    | 17/32 [00:00<00:00, 29.63it/s, loss=1.27e+3]wav2vec2_peft epoch 1/1:  53%|█████▎    | 17/32 [00:00<00:00, 29.63it/s, loss=1.78e+3]wav2vec2_peft epoch 1/1:  53%|█████▎    | 17/32 [00:00<00:00, 29.63it/s, loss=1.6e+3] wav2vec2_peft epoch 1/1:  53%|█████▎    | 17/32 [00:00<00:00, 29.63it/s, loss=1.38e+3]wav2vec2_peft epoch 1/1:  66%|██████▌   | 21/32 [00:00<00:00, 32.07it/s, loss=1.38e+3]wav2vec2_peft epoch 1/1:  66%|██████▌   | 21/32 [00:00<00:00, 32.07it/s, loss=1.11e+3]wav2vec2_peft epoch 1/1:  66%|██████▌   | 21/32 [00:00<00:00, 32.07it/s, loss=1.68e+3]wav2vec2_peft epoch 1/1:  66%|██████▌   | 21/32 [00:00<00:00, 32.07it/s, loss=1.31e+3]wav2vec2_peft epoch 1/1:  66%|██████▌   | 21/32 [00:00<00:00, 32.07it/s, loss=1.21e+3]wav2vec2_peft epoch 1/1:  78%|███████▊  | 25/32 [00:00<00:00, 32.03it/s, loss=1.21e+3]wav2vec2_peft epoch 1/1:  78%|███████▊  | 25/32 [00:00<00:00, 32.03it/s, loss=1.7e+3] wav2vec2_peft epoch 1/1:  78%|███████▊  | 25/32 [00:00<00:00, 32.03it/s, loss=1.39e+3]wav2vec2_peft epoch 1/1:  78%|███████▊  | 25/32 [00:00<00:00, 32.03it/s, loss=1.76e+3]wav2vec2_peft epoch 1/1:  78%|███████▊  | 25/32 [00:01<00:00, 32.03it/s, loss=1.22e+3]wav2vec2_peft epoch 1/1:  91%|█████████ | 29/32 [00:01<00:00, 32.12it/s, loss=1.22e+3]wav2vec2_peft epoch 1/1:  91%|█████████ | 29/32 [00:01<00:00, 32.12it/s, loss=1.19e+3]wav2vec2_peft epoch 1/1:  91%|█████████ | 29/32 [00:01<00:00, 32.12it/s, loss=1.71e+3]wav2vec2_peft epoch 1/1:  91%|█████████ | 29/32 [00:01<00:00, 32.12it/s, loss=1.69e+3]wav2vec2_peft epoch 1/1: 100%|██████████| 32/32 [00:01<00:00, 28.06it/s, loss=1.69e+3]
.venv/bin/python -m scripts.eval --config configs/smoke.yaml --out artifacts/results.json
/home/alireza/research-autopilot/generated_projects/streaming-asr-adapters-librispeech/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1118: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.
  super().__init__(loader)
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 and are newly initialized: ['wav2vec2.feature_extractor.conv_layers.1.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.1.layer_norm.weight', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 and are newly initialized: ['wav2vec2.feature_extractor.conv_layers.1.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.1.layer_norm.weight', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 and are newly initialized: ['wav2vec2.feature_extractor.conv_layers.1.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.1.layer_norm.weight', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 and are newly initialized: ['wav2vec2.feature_extractor.conv_layers.1.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.1.layer_norm.weight', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 and are newly initialized: ['wav2vec2.feature_extractor.conv_layers.1.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.1.layer_norm.weight', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 and are newly initialized: ['wav2vec2.feature_extractor.conv_layers.1.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.1.layer_norm.weight', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 and are newly initialized: ['wav2vec2.feature_extractor.conv_layers.1.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.1.layer_norm.weight', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 and are newly initialized: ['wav2vec2.feature_extractor.conv_layers.1.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.1.layer_norm.weight', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 and are newly initialized: ['wav2vec2.feature_extractor.conv_layers.1.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.1.layer_norm.weight', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 and are newly initialized: ['wav2vec2.feature_extractor.conv_layers.1.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.1.layer_norm.weight', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 and are newly initialized: ['wav2vec2.feature_extractor.conv_layers.1.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.1.layer_norm.weight', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at hf-internal-testing/tiny-random-wav2vec2 and are newly initialized: ['wav2vec2.feature_extractor.conv_layers.1.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.1.layer_norm.weight', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.bias', 'wav2vec2.feature_extractor.conv_layers.2.layer_norm.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
.venv/bin/python -m scripts.report --results artifacts/results.json --out artifacts/report.md
